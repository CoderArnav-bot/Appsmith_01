{
  "model": "llama-8b",
   "messages": {{JSONResponse1.chatHistory}},
    "max_tokens": 500,
    "temperature": 0.7,
    "top_p": 1,
    "stream": false
    }
}